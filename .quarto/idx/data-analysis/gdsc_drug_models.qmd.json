{"title":"Predicting drug resistance from gene expression","markdown":{"yaml":{"title":"Predicting drug resistance from gene expression","format":{"html":{"code-fold":true,"code-summary":"Show the code"}},"description":"As practice for using gradient boosted trees, I used an open dataset to use gene expression in tumours to predict resistance to chemotherapy drugs","date-format":"iso","date":"2024-04-02","image":"refametinib.png","categories":["data analysis","gdsc"],"execute":{"freeze":true}},"headingText":"It seems that previous releases get moved, so I'm just downloading it locally instead of getting it via ftp","containsRefs":false,"markdown":"\n\n```{python}\n#| warning: false\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lightgbm\nimport shap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nfrom re import sub\n```\nThis is a bit of work I did during my postdoc at the University of Nottingham. There's not enough here for a publication, and was a bit out of the scope of the project, so I'm writing it up here. I'm not reproducing the entirety of this work here, mostly because running the whole thing takes ages. The full workflow is still in the code, I have just commented parts of it out. If you do want to run the full analysis yourself, you can uncomment these lines and change a few variable names.\n\nI went to a conference^[The International Transmembrane Society Symposium] and heard a talk^[by Emily Barnes at the MRC Laboratory of Medical Sciences] in which the speaker described using a dataset of tumour cell lines to identify proteins important in cancer to identify a target to study.\nThe [Genomics of Drug Sensitivity in Cancer](https://www.cancerrxgene.org/) dataset is a massive project testing the sensitivity of tumour cell lines to different drugs. Part of the dataset is a table describing how sensitive cell lines are to each of a few hundred drugs.\n\n<!---\n- Expression to IC50\n- Expression IC50 adding weights\n- Expression IC50 transporters\n- Expression IC50 transporters optuna params\n- Filter IC50 by count\n- Explore GDSC1\n- Cluster gene density\n- Expression IC50 unclustered\n--->\n```{python}\n#ic50_data = pd.read_excel(\"ftp://ftp.sanger.ac.uk/pub/project/cancerrxgene/releases/current_release/GDSC1_fitted_dose_response_25Feb20.xlsx\")\nic50_data = pd.read_excel(\"data/GDSC1_fitted_dose_response_27Oct23.xlsx\")\nic50_data.drop(columns = ['NLME_RESULT_ID', 'NLME_CURVE_ID', 'SANGER_MODEL_ID', 'TCGA_DESC', 'DRUG_ID', 'PUTATIVE_TARGET', 'PATHWAY_NAME', 'COMPANY_ID', 'WEBRELEASE', 'MIN_CONC', 'MAX_CONC', 'AUC', 'RMSE'], inplace=True)\nic50_data.head()\n```\n\nI'm just using the first version of the dataset here, as shown by \"GDSC1\" in the `DATASET` column. There is a GDSC2 that could be added, but we have enough to be getting on with. The `COSMIC_ID` and `CELL_LINE_NAME` columns identify the cell line used in the experiment. `DRUG_NAME` should be fairly obvious.\n\nThe last two columns might be a bit less familiar. To explain these, I will have to be a little more precise about what \"sensitivity\" means. To create this dataset, the experimenters grew lots of samples of each cell line, and added different concentrations of drugs to the samples. They then let them grow for a bit, and counted how many were still viable (alive and able to keep going). They then analysed this data to see how much each concentration of the drug slowed the growth of cells. They then fitted this to a dose response curve to get an estimate of the concentration of the drug that inhibited growth by 50, a measure called the IC<sub>50</sub>. This measure is best described using a natural logarithm, so this is reported as `LN_IC50`. If you want to get a better feel for dose-response curves, I have a toy [here](making_a_dose-response_curve.qmd). The `Z_SCORE` describes how significant this difference is for that cell line, compared with the sensitivity of other cell lines to that drug.\n\nAnother part of the dataset is a measure of how genes are expressed in each cell line. For each of the cell lines in the dataset, the mRNA for each gene has been measured using a microarray.\n\n```{python}\n# I did have it downloading directly from the source, but I've had failures retrieving it and the IC50 data, so I'm loading it locally instead\n#rma_expr = pd.read_csv(\"https://www.cancerrxgene.org/gdsc1000/GDSC1000_WebResources//Data/preprocessed/Cell_line_RMA_proc_basalExp.txt.zip\", sep = \"\\t\")\nrma_expr = pd.read_csv(\"data/Cell_line_RMA_proc_basalExp.txt\", sep = \"\\t\")\nrma_expr = rma_expr.drop('GENE_title', axis = 1)\nrma_expr= rma_expr.set_index('GENE_SYMBOLS')\nrma_expr.head()\n```\n\nIn this table, each row is a gene, and each column shows the expression of that gene in a cell line. This data can be used with the drug sensitivity dataset using the `COSMIC_ID` column of that dataset. The column names in the expression table contain the COSMIC ID (`DATA.[COSMIC ID]`).\n\nWe can see how many of the cell lines described in the expression table are present in the sensitivity table.\n\n```{python}\nrma_cells = [int(x.split('.')[1]) for x in rma_expr.columns]\ncell_id_matches = []\n\nfor cell_id in rma_cells:\n    if cell_id in ic50_data['COSMIC_ID'].values:\n        cell_id_matches.append(cell_id)\n\nprint(f'Number of matches: {len(cell_id_matches)} of {len(rma_cells)}')\n```\n\nAnd filter the expression data so we're only using columns that are relevant to the cell lines of interest.\n\n```{python}\nrma_matches = np.isin(np.array(rma_cells), np.array(cell_id_matches))\n\nrma_expr_matched = rma_expr.iloc[:,rma_matches]\nrma_expr_matched.shape\n```\n\nThen carry out a similar operation on the sensitivity table\n\n```{python}\nic50_matched = ic50_data.loc[ic50_data['COSMIC_ID'].isin(cell_id_matches)]\nic50_matched.head()\n```\n\nHow many drugs does that leave us with?\n\n```{python}\nlen(ic50_matched['DRUG_NAME'].unique())\n```\n\nWhat's the plan here? A lot of statistical analysis has been carried out on this dataset by bioinformaticians much smarter than I am. What I want to get an idea of is how well this dataset can be used to predict drug sensitivity from expression data. In this first pass, I'm getting a baseline. There are things you can do to improve models like this, which I have carried out, but let's pretend, for the sake of narrative, that I haven't yet.\n\nTo use this data to train a predictive model, we really want a table where each row is a cell line, and each column is a gene.\n\n```{python}\nrma_expr_matched = rma_expr_matched.T\nrma_expr_matched['COSMIC_ID'] = [int(x.split('.')[1]) for x in rma_expr_matched.index]\nrma_expr_matched = rma_expr_matched.loc[:, rma_expr_matched.columns.notna()]\nrma_expr_matched.head()\n```\n\nI'm going to use a kind of model called [\"Gradient Boosted Trees\"](https://en.wikipedia.org/wiki/Gradient_boosting) to predict drug sensitivity here. I won't go into too much detail on how they work (Google has a course on them [here](https://developers.google.com/machine-learning/decision-forests)), but essentially they build up a model from a set of small models called decision trees. It starts with a simple decision tree, then adds another simple decision tree to the output of that one that is chosen to best deal with the shortcomings of the previous. This process is repeated until adding more trees doesn't improve prediction very much. These models perform really well on small, structured datasets, so are a good choice here. The implementation I'm using is called [LightGBM](https://lightgbm.readthedocs.io/en/stable/), which, from my understanding, makes models that are nearly as good as gradient boosted trees can be, but are relatively fast.\n\nFor each drug, my `model_drug` function takes the IC<sub>50</sub> data matching that drug, then merges that data with matching data from the expression table.\n\nIt then splits this into a training and test set. The training set contains 80% of the data. The decision trees are built based on this data, then the model's performance is assessed on how well it predicts the IC<sub>50</sub> in the remaining 20%.\n\nIn my first run at this, I trained a model for each of the drugs in the dataset. To do this, I had to start it when I got in one morning, go do a day's work in the lab, then come back to collect the results. Here, I've just chosen 14 drugs that (spoiler alert) do well here, and one that I know doesn't. If you want to check my work, the code for running the whole lot is still in there.\n\nFor each drug, the Mean Absolute Error of the model and the range of ln(IC<sub>50</sub>) in the test set are reported.\n\n```{python}\n#| warning: false\ndef model_drug(drug, verbose = False, figure = False):\n    ic50_sub = ic50_matched.loc[ic50_matched['DRUG_NAME'] == drug][['COSMIC_ID',\n                                                                    'LN_IC50',\n                                                                    'Z_SCORE']]\n    df = pd.merge(ic50_sub, rma_expr_matched).set_index('COSMIC_ID')\n\n    X = df.drop(['LN_IC50', 'Z_SCORE'], axis = 1)\n    y = df['LN_IC50']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\n    train_data = lightgbm.Dataset(X_train, label = y_train)\n    test_data = lightgbm.Dataset(X_test, label = y_test, reference = train_data)\n\n    param = {'boosting_type': 'goss',\n             'n_estimators': 500,\n             'num_iterations': 500,\n             'learning_rate': 0.05,\n             'max_bin': 1024,\n             'metric': 'l2',\n             'objective': 'regression',\n             'num_leaves': 50,\n             'verbose': -1}\n\n    bst = lightgbm.train(param,\n                         train_data,\n                         callbacks=[lightgbm.early_stopping(stopping_rounds=30, verbose = False)],\n                         valid_sets = test_data)\n\n    fit_predict = bst.predict(X_test)\n    if verbose:\n        mae = mean_absolute_error(fit_predict, y_test)\n        test_range = max(y_test)-min(y_test)\n        print(f'{drug}:\\nMAE = {mae:.3} (range {test_range:.3})')\n    if figure:\n        fig, ax = plt.subplots(figsize = (16,8), ncols = 2)\n        ax[0].scatter(y_test, fit_predict, color = 'black', alpha = 0.5)\n        ax[0].ylabel = 'Predicted ln(IC50)'\n        ax[0].xlabel = 'True ln(IC50)'\n        ax[0].set_title(drug)\n        lightgbm.plot_importance(bst, max_num_features=20, ax = ax[1])\n        if figure == 'save':\n            filename = sub('[^A-Za-z0-9-]+', '', drug)\n            plt.savefig(f'{filename}.png')\n    return bst, fit_predict, y_test\n\n\n#all_models = dict()\n\n#for drug in ic50_matched['DRUG_NAME'].unique():\n#    all_models[drug] = model_drug(drug)\n\nexample_drugs = ['AZ628', 'WZ3105', 'NPK76-II-72-1', 'Tubastatin A', 'PIK-93', 'Venotoclax', 'Methotrexate', 'Refametinib', 'AZD7762', 'Tanespimycin', 'Nutlin-3a (-)', 'Trametinib', 'Dabrafenib','SN-38', 'Erlotinib']\n\nexample_models = dict()\n\nfor drug in example_drugs:\n  example_models[drug] = model_drug(drug, verbose=True)\n```\n\nThe R<sup>2</sup> value describes how much of the variance of a dependent variable can be explained by the independent variable. I'm using it here to get an idea of how well the predictions from the models correlate with the observations.\n\n```{python}\ndef r_squared(predicted, true):\n    mean = np.mean(true)\n    true_diff_sq = np.square(true - mean)\n    pred_diff_sq = np.square(true - predicted)\n    return 1-(np.sum(pred_diff_sq)/np.sum(true_diff_sq))\n\nmodels_r_sq = dict([(x, r_squared(y[1], y[2])) for x, y in example_models.items()])\n\n[(x,y) for x,y in models_r_sq.items() if y > 0.4]\n```\n\nNot bad for a first go. Here's what a plot of the predicted vs observed values looks like for Refametinib, which can be predicted pretty well.\n\n```{python}\ndef plot_test(drug):\n    bst, fit_predict, y_test = example_models[drug]\n    fig, ax = plt.subplots(figsize = (16,8), ncols = 2)\n    ax[0].scatter(y_test, fit_predict, color = 'black', alpha = 0.5)\n    ax[0].ylabel = 'Predicted ln(IC50)'\n    ax[0].xlabel = 'True ln(IC50)'\n    ax[0].set_title(drug)\n    lightgbm.plot_importance(bst, max_num_features=20, ax = ax[1])\n\nplot_test('Refametinib')\n```\n\nAnd for Erlotinib, one that can't (in this first go, at least).\n\n```{python}\nplot_test('Erlotinib')\n```\n\nThe panels on the left of these plots show this comparison. Hopefully, you'll be wondering what the other panel shows. A neat feature of LightGBM models is that the model can tell you how important different dependent variables are when making a prediction. This means that we can get an idea of how the differential expression of particular genes contributes to the sensitivity of tumour cells to drugs^[to be properly rigourous, we can't really: correlation doesn't imply causation! Without properly designed experiments this is all speculation].\n\nTo get a little preview of what I'll do later in^[what will hopefully be] this series, there's a more powerful way of looking at how different variables affect prediction. This is using something called SHAP (SHapley Additive exPlanations) values. I'll go more into these in another post, but here's a plot of SHAP values for the model trying to predict AZ628 sensitivity.\n\n```{python}\nshap.initjs()\n```\n\nThe colour of a point describes how high the feature (dependent variable) value is, and the x-position is the SHAP value. For example, a high DUSP6 expression value is associated with a more negative SHAP value, meaning a lower IC<sub>50</sub>, so the cell line is more sensitive to AZ628. There's a lot more that can be done with these, but I think this is quite long enough for one post.\n\n```{python}\n#| warning: false\ndef prep_data(drug):\n    ic50_sub = ic50_matched.loc[ic50_matched['DRUG_NAME'] == drug][['COSMIC_ID',\n                                                                    'LN_IC50',\n                                                                    'Z_SCORE']]\n    df = pd.merge(ic50_sub, rma_expr_matched).set_index('COSMIC_ID')\n\n    return df.drop(['LN_IC50', 'Z_SCORE'], axis = 1)\n\ndef plot_shap(drug):\n    model = example_models[drug][0]\n    X = prep_data(drug)\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    shap.summary_plot(shap_values, X, title = drug)\n\nplot_shap('AZ628')\n```\n\nIn subsequent posts, I'll be aiming to improve model performance through various avenues, and trying to find whether we can actually make any useful interpretations of these models. They're quite promising. For Refametinib, the model correlates really well with the observed data. This doesn't take into account any factors other than changes in expression, so is quite impressive!\n","srcMarkdownNoYaml":"\n\n```{python}\n#| warning: false\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lightgbm\nimport shap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nfrom re import sub\n```\nThis is a bit of work I did during my postdoc at the University of Nottingham. There's not enough here for a publication, and was a bit out of the scope of the project, so I'm writing it up here. I'm not reproducing the entirety of this work here, mostly because running the whole thing takes ages. The full workflow is still in the code, I have just commented parts of it out. If you do want to run the full analysis yourself, you can uncomment these lines and change a few variable names.\n\nI went to a conference^[The International Transmembrane Society Symposium] and heard a talk^[by Emily Barnes at the MRC Laboratory of Medical Sciences] in which the speaker described using a dataset of tumour cell lines to identify proteins important in cancer to identify a target to study.\nThe [Genomics of Drug Sensitivity in Cancer](https://www.cancerrxgene.org/) dataset is a massive project testing the sensitivity of tumour cell lines to different drugs. Part of the dataset is a table describing how sensitive cell lines are to each of a few hundred drugs.\n\n<!---\n- Expression to IC50\n- Expression IC50 adding weights\n- Expression IC50 transporters\n- Expression IC50 transporters optuna params\n- Filter IC50 by count\n- Explore GDSC1\n- Cluster gene density\n- Expression IC50 unclustered\n--->\n```{python}\n# It seems that previous releases get moved, so I'm just downloading it locally instead of getting it via ftp\n#ic50_data = pd.read_excel(\"ftp://ftp.sanger.ac.uk/pub/project/cancerrxgene/releases/current_release/GDSC1_fitted_dose_response_25Feb20.xlsx\")\nic50_data = pd.read_excel(\"data/GDSC1_fitted_dose_response_27Oct23.xlsx\")\nic50_data.drop(columns = ['NLME_RESULT_ID', 'NLME_CURVE_ID', 'SANGER_MODEL_ID', 'TCGA_DESC', 'DRUG_ID', 'PUTATIVE_TARGET', 'PATHWAY_NAME', 'COMPANY_ID', 'WEBRELEASE', 'MIN_CONC', 'MAX_CONC', 'AUC', 'RMSE'], inplace=True)\nic50_data.head()\n```\n\nI'm just using the first version of the dataset here, as shown by \"GDSC1\" in the `DATASET` column. There is a GDSC2 that could be added, but we have enough to be getting on with. The `COSMIC_ID` and `CELL_LINE_NAME` columns identify the cell line used in the experiment. `DRUG_NAME` should be fairly obvious.\n\nThe last two columns might be a bit less familiar. To explain these, I will have to be a little more precise about what \"sensitivity\" means. To create this dataset, the experimenters grew lots of samples of each cell line, and added different concentrations of drugs to the samples. They then let them grow for a bit, and counted how many were still viable (alive and able to keep going). They then analysed this data to see how much each concentration of the drug slowed the growth of cells. They then fitted this to a dose response curve to get an estimate of the concentration of the drug that inhibited growth by 50, a measure called the IC<sub>50</sub>. This measure is best described using a natural logarithm, so this is reported as `LN_IC50`. If you want to get a better feel for dose-response curves, I have a toy [here](making_a_dose-response_curve.qmd). The `Z_SCORE` describes how significant this difference is for that cell line, compared with the sensitivity of other cell lines to that drug.\n\nAnother part of the dataset is a measure of how genes are expressed in each cell line. For each of the cell lines in the dataset, the mRNA for each gene has been measured using a microarray.\n\n```{python}\n# I did have it downloading directly from the source, but I've had failures retrieving it and the IC50 data, so I'm loading it locally instead\n#rma_expr = pd.read_csv(\"https://www.cancerrxgene.org/gdsc1000/GDSC1000_WebResources//Data/preprocessed/Cell_line_RMA_proc_basalExp.txt.zip\", sep = \"\\t\")\nrma_expr = pd.read_csv(\"data/Cell_line_RMA_proc_basalExp.txt\", sep = \"\\t\")\nrma_expr = rma_expr.drop('GENE_title', axis = 1)\nrma_expr= rma_expr.set_index('GENE_SYMBOLS')\nrma_expr.head()\n```\n\nIn this table, each row is a gene, and each column shows the expression of that gene in a cell line. This data can be used with the drug sensitivity dataset using the `COSMIC_ID` column of that dataset. The column names in the expression table contain the COSMIC ID (`DATA.[COSMIC ID]`).\n\nWe can see how many of the cell lines described in the expression table are present in the sensitivity table.\n\n```{python}\nrma_cells = [int(x.split('.')[1]) for x in rma_expr.columns]\ncell_id_matches = []\n\nfor cell_id in rma_cells:\n    if cell_id in ic50_data['COSMIC_ID'].values:\n        cell_id_matches.append(cell_id)\n\nprint(f'Number of matches: {len(cell_id_matches)} of {len(rma_cells)}')\n```\n\nAnd filter the expression data so we're only using columns that are relevant to the cell lines of interest.\n\n```{python}\nrma_matches = np.isin(np.array(rma_cells), np.array(cell_id_matches))\n\nrma_expr_matched = rma_expr.iloc[:,rma_matches]\nrma_expr_matched.shape\n```\n\nThen carry out a similar operation on the sensitivity table\n\n```{python}\nic50_matched = ic50_data.loc[ic50_data['COSMIC_ID'].isin(cell_id_matches)]\nic50_matched.head()\n```\n\nHow many drugs does that leave us with?\n\n```{python}\nlen(ic50_matched['DRUG_NAME'].unique())\n```\n\nWhat's the plan here? A lot of statistical analysis has been carried out on this dataset by bioinformaticians much smarter than I am. What I want to get an idea of is how well this dataset can be used to predict drug sensitivity from expression data. In this first pass, I'm getting a baseline. There are things you can do to improve models like this, which I have carried out, but let's pretend, for the sake of narrative, that I haven't yet.\n\nTo use this data to train a predictive model, we really want a table where each row is a cell line, and each column is a gene.\n\n```{python}\nrma_expr_matched = rma_expr_matched.T\nrma_expr_matched['COSMIC_ID'] = [int(x.split('.')[1]) for x in rma_expr_matched.index]\nrma_expr_matched = rma_expr_matched.loc[:, rma_expr_matched.columns.notna()]\nrma_expr_matched.head()\n```\n\nI'm going to use a kind of model called [\"Gradient Boosted Trees\"](https://en.wikipedia.org/wiki/Gradient_boosting) to predict drug sensitivity here. I won't go into too much detail on how they work (Google has a course on them [here](https://developers.google.com/machine-learning/decision-forests)), but essentially they build up a model from a set of small models called decision trees. It starts with a simple decision tree, then adds another simple decision tree to the output of that one that is chosen to best deal with the shortcomings of the previous. This process is repeated until adding more trees doesn't improve prediction very much. These models perform really well on small, structured datasets, so are a good choice here. The implementation I'm using is called [LightGBM](https://lightgbm.readthedocs.io/en/stable/), which, from my understanding, makes models that are nearly as good as gradient boosted trees can be, but are relatively fast.\n\nFor each drug, my `model_drug` function takes the IC<sub>50</sub> data matching that drug, then merges that data with matching data from the expression table.\n\nIt then splits this into a training and test set. The training set contains 80% of the data. The decision trees are built based on this data, then the model's performance is assessed on how well it predicts the IC<sub>50</sub> in the remaining 20%.\n\nIn my first run at this, I trained a model for each of the drugs in the dataset. To do this, I had to start it when I got in one morning, go do a day's work in the lab, then come back to collect the results. Here, I've just chosen 14 drugs that (spoiler alert) do well here, and one that I know doesn't. If you want to check my work, the code for running the whole lot is still in there.\n\nFor each drug, the Mean Absolute Error of the model and the range of ln(IC<sub>50</sub>) in the test set are reported.\n\n```{python}\n#| warning: false\ndef model_drug(drug, verbose = False, figure = False):\n    ic50_sub = ic50_matched.loc[ic50_matched['DRUG_NAME'] == drug][['COSMIC_ID',\n                                                                    'LN_IC50',\n                                                                    'Z_SCORE']]\n    df = pd.merge(ic50_sub, rma_expr_matched).set_index('COSMIC_ID')\n\n    X = df.drop(['LN_IC50', 'Z_SCORE'], axis = 1)\n    y = df['LN_IC50']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\n    train_data = lightgbm.Dataset(X_train, label = y_train)\n    test_data = lightgbm.Dataset(X_test, label = y_test, reference = train_data)\n\n    param = {'boosting_type': 'goss',\n             'n_estimators': 500,\n             'num_iterations': 500,\n             'learning_rate': 0.05,\n             'max_bin': 1024,\n             'metric': 'l2',\n             'objective': 'regression',\n             'num_leaves': 50,\n             'verbose': -1}\n\n    bst = lightgbm.train(param,\n                         train_data,\n                         callbacks=[lightgbm.early_stopping(stopping_rounds=30, verbose = False)],\n                         valid_sets = test_data)\n\n    fit_predict = bst.predict(X_test)\n    if verbose:\n        mae = mean_absolute_error(fit_predict, y_test)\n        test_range = max(y_test)-min(y_test)\n        print(f'{drug}:\\nMAE = {mae:.3} (range {test_range:.3})')\n    if figure:\n        fig, ax = plt.subplots(figsize = (16,8), ncols = 2)\n        ax[0].scatter(y_test, fit_predict, color = 'black', alpha = 0.5)\n        ax[0].ylabel = 'Predicted ln(IC50)'\n        ax[0].xlabel = 'True ln(IC50)'\n        ax[0].set_title(drug)\n        lightgbm.plot_importance(bst, max_num_features=20, ax = ax[1])\n        if figure == 'save':\n            filename = sub('[^A-Za-z0-9-]+', '', drug)\n            plt.savefig(f'{filename}.png')\n    return bst, fit_predict, y_test\n\n\n#all_models = dict()\n\n#for drug in ic50_matched['DRUG_NAME'].unique():\n#    all_models[drug] = model_drug(drug)\n\nexample_drugs = ['AZ628', 'WZ3105', 'NPK76-II-72-1', 'Tubastatin A', 'PIK-93', 'Venotoclax', 'Methotrexate', 'Refametinib', 'AZD7762', 'Tanespimycin', 'Nutlin-3a (-)', 'Trametinib', 'Dabrafenib','SN-38', 'Erlotinib']\n\nexample_models = dict()\n\nfor drug in example_drugs:\n  example_models[drug] = model_drug(drug, verbose=True)\n```\n\nThe R<sup>2</sup> value describes how much of the variance of a dependent variable can be explained by the independent variable. I'm using it here to get an idea of how well the predictions from the models correlate with the observations.\n\n```{python}\ndef r_squared(predicted, true):\n    mean = np.mean(true)\n    true_diff_sq = np.square(true - mean)\n    pred_diff_sq = np.square(true - predicted)\n    return 1-(np.sum(pred_diff_sq)/np.sum(true_diff_sq))\n\nmodels_r_sq = dict([(x, r_squared(y[1], y[2])) for x, y in example_models.items()])\n\n[(x,y) for x,y in models_r_sq.items() if y > 0.4]\n```\n\nNot bad for a first go. Here's what a plot of the predicted vs observed values looks like for Refametinib, which can be predicted pretty well.\n\n```{python}\ndef plot_test(drug):\n    bst, fit_predict, y_test = example_models[drug]\n    fig, ax = plt.subplots(figsize = (16,8), ncols = 2)\n    ax[0].scatter(y_test, fit_predict, color = 'black', alpha = 0.5)\n    ax[0].ylabel = 'Predicted ln(IC50)'\n    ax[0].xlabel = 'True ln(IC50)'\n    ax[0].set_title(drug)\n    lightgbm.plot_importance(bst, max_num_features=20, ax = ax[1])\n\nplot_test('Refametinib')\n```\n\nAnd for Erlotinib, one that can't (in this first go, at least).\n\n```{python}\nplot_test('Erlotinib')\n```\n\nThe panels on the left of these plots show this comparison. Hopefully, you'll be wondering what the other panel shows. A neat feature of LightGBM models is that the model can tell you how important different dependent variables are when making a prediction. This means that we can get an idea of how the differential expression of particular genes contributes to the sensitivity of tumour cells to drugs^[to be properly rigourous, we can't really: correlation doesn't imply causation! Without properly designed experiments this is all speculation].\n\nTo get a little preview of what I'll do later in^[what will hopefully be] this series, there's a more powerful way of looking at how different variables affect prediction. This is using something called SHAP (SHapley Additive exPlanations) values. I'll go more into these in another post, but here's a plot of SHAP values for the model trying to predict AZ628 sensitivity.\n\n```{python}\nshap.initjs()\n```\n\nThe colour of a point describes how high the feature (dependent variable) value is, and the x-position is the SHAP value. For example, a high DUSP6 expression value is associated with a more negative SHAP value, meaning a lower IC<sub>50</sub>, so the cell line is more sensitive to AZ628. There's a lot more that can be done with these, but I think this is quite long enough for one post.\n\n```{python}\n#| warning: false\ndef prep_data(drug):\n    ic50_sub = ic50_matched.loc[ic50_matched['DRUG_NAME'] == drug][['COSMIC_ID',\n                                                                    'LN_IC50',\n                                                                    'Z_SCORE']]\n    df = pd.merge(ic50_sub, rma_expr_matched).set_index('COSMIC_ID')\n\n    return df.drop(['LN_IC50', 'Z_SCORE'], axis = 1)\n\ndef plot_shap(drug):\n    model = example_models[drug][0]\n    X = prep_data(drug)\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    shap.summary_plot(shap_values, X, title = drug)\n\nplot_shap('AZ628')\n```\n\nIn subsequent posts, I'll be aiming to improve model performance through various avenues, and trying to find whether we can actually make any useful interpretations of these models. They're quite promising. For Refametinib, the model correlates really well with the observed data. This doesn't take into account any factors other than changes in expression, so is quite impressive!\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"gdsc_drug_models.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.552","theme":"flatly","title":"Predicting drug resistance from gene expression","description":"As practice for using gradient boosted trees, I used an open dataset to use gene expression in tumours to predict resistance to chemotherapy drugs","date-format":"iso","date":"2024-04-02","image":"refametinib.png","categories":["data analysis","gdsc"],"code-summary":"Show the code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}